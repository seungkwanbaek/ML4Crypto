{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Problem\n",
    "\n",
    "Kickstarter success rate. \n",
    "\n",
    "\n",
    "Training and Testing error. Training time. ROC curve\n",
    "\n",
    "\n",
    "\n",
    "2. the training and testing error rates you obtained running the various learning algorithms on your problems. At the very least you should include graphs that show performance on both training and test data as a function of training size (note that this implies that you need to design a classification problem that has more than a trivial amount of data) and--for the algorithms that are iterative--training times/iterations. Both of these kinds of graphs are referred to as learning curves, BTW.\n",
    "\n",
    "3. analyses of your results. Why did you get the results you did? Compare and contrast the different algorithms. What sort of changes might you make to each of those algorithms to improve performance? How fast were they in terms of wall clock time? Iterations? Would cross validation help (and if it would, why didn't you implement it?)? How much performance was due to the problems you chose? How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can.\n",
    "\n",
    "\n",
    "-Why did you get the results you did? \n",
    "\n",
    "-Compare and contrast the different algorithms. \n",
    "\n",
    "-What sort of changes might you make to each of those algorithms to improve performance? \n",
    "\n",
    "-How fast were they in terms of wall clock time? Iterations? \n",
    "\n",
    "-Would cross validation help (and if it would, why didn't you implement it?)? How much performance was due to the \n",
    "problems you chose? \n",
    "\n",
    "-How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? \n",
    "\n",
    "-Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('BTCUSDKraken.csv', index_col=0, usecols=['Date','Volume (BTC)', 'Weighted Price'])\n",
    "df = df.drop('2018-01-12')\n",
    "df = df.replace([0], np.nan)\n",
    "df = df.ffill(axis=None, inplace=False, limit=None, downcast=None)\n",
    "lookback = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryanbaek/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "  \n",
      "/Users/bryanbaek/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "delta = df['Weighted Price'].diff()\n",
    "dUp, dDown = delta.copy(), delta.copy()\n",
    "dUp[dUp < 0] = 0\n",
    "dDown[dDown > 0] = 0\n",
    "\n",
    "RolUp = pd.rolling_mean(dUp, lookback)\n",
    "RolDown = pd.rolling_mean(dDown, lookback).abs()\n",
    "\n",
    "df['RSI'] = RolUp / RolDown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryanbaek/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=3,window=3,center=False).mean()\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/bryanbaek/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.ewm(ignore_na=False,span=3,min_periods=3,adjust=True).mean()\n",
      "  \n",
      "/Users/bryanbaek/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(min_periods=3,window=3,center=False).std()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "sma = pd.rolling_mean(df['Weighted Price'], window = lookback, min_periods = lookback)\n",
    "ema = pd.ewma(df['Weighted Price'], span = lookback, min_periods = lookback)\n",
    "rolling_std = pd.rolling_std(df['Weighted Price'], window = lookback, min_periods = lookback)\n",
    "up = (rolling_std * 2) + ema\n",
    "down = (rolling_std * -2) + ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['Price/SMA'] = df['Weighted Price']/sma\n",
    "df['bbp'] = (df['Weighted Price'] - down) / (up - down)\n",
    "df['Price/EMA'] = df['Weighted Price'] / ema\n",
    "df['price_momentum'] = df['Weighted Price'].pct_change(periods = lookback - 1)\n",
    "df['vol_momentum'] = df['Volume (BTC)'].pct_change(periods = lookback - 1)\n",
    "df['bestaction'] = df['Weighted Price'].pct_change(periods = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descending order from the Upper Band:\n",
    "\n",
    "%B Above 1 = Price is Above the Upper Band\n",
    "\n",
    "%B Equal to 1 = Price is at the Upper Band\n",
    "\n",
    "%B Above .50 = Price is Above the Middle Line\n",
    "\n",
    "%B Below .50 = Price is Below the Middle Line\n",
    "\n",
    "%B Equal to 0 = Price is at the Lower Band\n",
    "\n",
    "%B Below 0 = Price is Below the Lower Band\n",
    "\n",
    "Generally speaking .80 and .20 are also relevant levels.\n",
    "\n",
    "%B Above .80 = Price is Nearing the Upper Band\n",
    "%B Below .20 = Price is Nearing the Lower Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suggestion_conditions = [\n",
    "    (df['Price/EMA'] < 0.97) & (df['bbp'] < 0.25),\n",
    "    (df['Price/EMA'] > 1.01) & (df['bbp'] > 0.75)]\n",
    "\n",
    "actual_conditions = [\n",
    "    (df['bestaction'] > 0), (df['bestaction'] < 0) ]\n",
    "\n",
    "choices = [1, -1]\n",
    "#df['Suggestion'] = np.select(suggestion_conditions, choices, default=0)\n",
    "df['Action'] = np.select(actual_conditions, choices, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Volume (BTC)', 'Weighted Price', 'bestaction'], axis=1)\n",
    "df = df.drop(df.index[0:lookback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(df.columns[0:-1])\n",
    "X = df[features]\n",
    "Y = df['Action']\n",
    "\n",
    "#Shuffle = False b/c gotta train on earlier data and test on later data \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=5, \n",
    "                                  min_samples_leaf=2, min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                  min_impurity_split=None, class_weight=None, presort=False)\n",
    "#DT = DT.fit(X_train, Y_train)\n",
    "#Ypreds = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84        0.87878788  0.76767677  0.7979798   0.80808081  0.80808081\n",
      "  0.85714286  0.82653061]\n",
      "Accuracy: 0.82 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "#The least populated class in y has only 8 members, which is too few. \n",
    "#The minimum number of members in any class cannot be less than n_splits=9.\n",
    "scores = cross_val_score(DT, X_train, Y_train, cv = 8)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning:\n",
    "\n",
    "min_samples_split : int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Networks (Multi-layer Perceptron (MLP) )\n",
    "\n",
    "Pro:\n",
    "*Capability to learn non-linear models.\n",
    "\n",
    "Con: \n",
    "*MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "\n",
    "*MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "\n",
    "*MLP is sensitive to feature scaling.\n",
    "\n",
    "\n",
    "## Features:\n",
    "1. hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
    "\n",
    "The ith element represents the number of neurons in the ith hidden layer.\n",
    "hidden_layer_sizes=(10, 2) : 10 hidden layer with 2 hidden units\n",
    "\n",
    "2. activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "Activation function for the hidden layer.\n",
    "\n",
    "3. solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "The solver for weight optimization.\n",
    "\n",
    "4. learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’\n",
    "Learning rate schedule for weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(hidden_layer_sizes=(20, 2), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n",
    " learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    " random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    " early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "#NN = NN.fit(X_train, Y_train) \n",
    "#Ypreds = NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5625      0.425       0.5625      0.0125      0.56962025  0.58227848\n",
      "  0.83544304  0.60759494  0.57692308  0.42857143]\n",
      "Accuracy: 0.52 (+/- 0.40)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NN, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boosting (Random Forest)\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "\n",
    "Pros:\n",
    "\n",
    "* It runs efficiently on large data bases.\n",
    "* It can handle thousands of input variables without variable deletion.\n",
    "* It gives estimates of what variables are important in the classification.\n",
    "* It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "* It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "* It has methods for balancing error in class population unbalanced data sets.\n",
    "* Generated forests can be saved for future use on other data.\n",
    "* Prototypes are computed that give information about the relation between the variables and the classification.\n",
    "* It computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.\n",
    "* The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "* It offers an experimental method for detecting variable interactions.\n",
    "Number of Trees in RF\n",
    "* Random forests does not overfit. You can run as many trees as you want. * It is fast. Running on a data set with 50,000 cases and 100 variables, it produced 100 trees in 11 minutes on a 800Mhz machine. For large data sets the major memory requirement is the storage of the data itself, and three integer arrays with the same dimensions as the data. If proximities are calculated, storage requirements grow as the number of cases times the number of trees.\n",
    "\n",
    "* In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n",
    "\n",
    "* Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.\n",
    "\n",
    "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks\n",
    "The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations z_i = (x_i, y_i).\n",
    "\n",
    "The out-of-bag (OOB) error is the average error for each z_i calculated using predictions from the trees that do not contain z_i in their respective bootstrap sample.\n",
    "\n",
    "This allows the RandomForestClassifier to be fit and validated whilst being trained.\n",
    "\n",
    "T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, p592-593, Springer, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "1. n_estimators : integer, optional (default=10)\n",
    "The number of trees in the forest.\n",
    "\n",
    "2. criterion : string, optional (default=”gini”)\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "\n",
    "3. max_features : int, float, string or None, optional (default=”auto”)\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "4. max_depth : integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "5. min_samples_split : int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node\n",
    "\n",
    "6. min_samples_leaf : int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.875       0.8625      0.85        0.825       0.78481013  0.86075949\n",
      "  0.87341772  0.83544304  0.83333333  0.81818182]\n",
      "Accuracy: 0.84 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=10, criterion='gini', \n",
    "                            max_depth=None, min_samples_split=2, \n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                            max_features='auto', max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                            bootstrap=True, oob_score=False, n_jobs=1, \n",
    "                            random_state=None, verbose=0, \n",
    "                            warm_start=False, class_weight=None)\n",
    "\n",
    "#RF = RF.fit(X_train, Y_train) \n",
    "#Ypreds = RF.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(RF, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-at least 2 kernel functions\n",
    "\n",
    "Pros: \n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n",
    "\n",
    "## Features:\n",
    "\n",
    "1. Kernel(Linear, RBF, poly, sigmoid, precomputed)\n",
    "\n",
    "Linear kernel = Straight Line (hyperplane) as the decision boundary\n",
    "* rarely used in practice\n",
    "\n",
    "Radial Basis Function (RBF) = commonly used kernel in SVC\n",
    "2 parameters:\n",
    "* gamma\n",
    "* C\n",
    "\n",
    "2. Gamma:\n",
    "*  'spread' of the kernel and therefore the decision region.\n",
    "* low gamma -> the 'curve' of the decision boundary is very low and thus the decision region is very broad (underfitting)\n",
    "* gamma = 10 (The decision boundary starts to be highly effected by individual data points (i.e. variance)).\n",
    "* high gamma -> the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points (overfitting)\n",
    "* If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "* gamma defines how much influence a single training example has. \n",
    "* The larger gamma is, the closer other examples must be to be affected.\n",
    "\n",
    "3. C:\n",
    "* trades off misclassification of training examples against simplicity of the decision surface\n",
    "* A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly\n",
    "* penalty for misclassifying a data point\n",
    "* small C -> classifier is okay with misclassified data points (high bias, low variance)\n",
    "* big C -> classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)\n",
    "\n",
    "C > 10 is too slow\n",
    "\n",
    "4. degree : int, optional (default=3)\n",
    "* Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "[ 0.825       0.8125      0.725       0.7375      0.7721519   0.82278481\n",
      "  0.83544304  0.69620253  0.76923077  0.79220779]\n",
      "Accuracy: 0.78 (+/- 0.09)\n",
      "Kernel: rbf\n",
      "[ 0.7625      0.8375      0.7375      0.6875      0.72151899  0.70886076\n",
      "  0.83544304  0.70886076  0.79487179  0.77922078]\n",
      "Accuracy: 0.76 (+/- 0.10)\n",
      "Kernel: sigmoid\n",
      "[ 0.575       0.5125      0.4375      0.4375      0.49367089  0.48101266\n",
      "  0.51898734  0.44303797  0.52564103  0.5974026 ]\n",
      "Accuracy: 0.50 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "#takes FOREVER\n",
    "#linear 0.78\n",
    "#rbf 0.76\n",
    "#sigmoid 0.5\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "for i in kernels:\n",
    "    #for j, C in enumerate((0.01, 1, 10, 100)):\n",
    "        #for k, D in enumerate((1, 10)):\n",
    "    SVM = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "              decision_function_shape='ovr', degree=3, gamma='auto', kernel=i,\n",
    "              max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "    #SVM.fit(X_train, Y_train) \n",
    "    scores = cross_val_score(SVM, X_train, Y_train, cv = 10)\n",
    "\n",
    "    #print (\"Kernel: %s | Gamma: %0.2f | C: %i\" % (i, C, D))\n",
    "    print (\"Kernel: %s\" % (i))\n",
    "    print scores\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8         0.7875      0.675       0.65        0.67088608  0.7721519\n",
      "  0.79746835  0.7721519   0.75641026  0.75324675]\n",
      "Accuracy: 0.74 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "                           leaf_size=30, p=2, metric='minkowski', metric_params=None, \n",
    "                           n_jobs=1)\n",
    "#KNN.fit(X_train, Y_train)\n",
    "scores = cross_val_score(KNN, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "1. weights : str or callable, optional (default = ‘uniform’)\n",
    "weight function used in prediction. Possible values:\n",
    "\n",
    "\n",
    "* uniform : uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "* distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Adjusted R Square\n",
    "\n",
    "ROC curve\n",
    "\n",
    "auc Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"Decision Tree\", \"Neural Network\", \"Boosting\", \"SVM\", \"KNN\"]\n",
    "\n",
    "#implement ROC Curves\n",
    "#\n",
    "\n",
    "h = .02 \n",
    "\n",
    "classifiers = [DT, NN, RF, SVM, KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
