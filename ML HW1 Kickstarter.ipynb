{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Problem\n",
    "\n",
    "Kickstarter success rate. \n",
    "\n",
    "\n",
    "Training and Testing error. Training time. ROC curve\n",
    "\n",
    "\n",
    "\n",
    "2. the training and testing error rates you obtained running the various learning algorithms on your problems. At the very least you should include graphs that show performance on both training and test data as a function of training size (note that this implies that you need to design a classification problem that has more than a trivial amount of data) and--for the algorithms that are iterative--training times/iterations. Both of these kinds of graphs are referred to as learning curves, BTW.\n",
    "\n",
    "3. analyses of your results. Why did you get the results you did? Compare and contrast the different algorithms. What sort of changes might you make to each of those algorithms to improve performance? How fast were they in terms of wall clock time? Iterations? Would cross validation help (and if it would, why didn't you implement it?)? How much performance was due to the problems you chose? How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can.\n",
    "\n",
    "\n",
    "-Why did you get the results you did? \n",
    "\n",
    "-Compare and contrast the different algorithms. \n",
    "\n",
    "-What sort of changes might you make to each of those algorithms to improve performance? \n",
    "\n",
    "-How fast were they in terms of wall clock time? Iterations? \n",
    "\n",
    "-Would cross validation help (and if it would, why didn't you implement it?)? How much performance was due to the \n",
    "problems you chose? \n",
    "\n",
    "-How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? \n",
    "\n",
    "-Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Kickstarter.csv')\n",
    "df = df.drop(['ID', 'name'], axis=1)\n",
    "df['deadline'] = pd.to_datetime(df['deadline'])\n",
    "df['launched'] = pd.to_datetime(df['launched'])\n",
    "df['duration'] = df['deadline'] - df['launched']\n",
    "df['pledged_percentage'] = df['pledged']/df['goal']\n",
    "actual_conditions = [\n",
    "    (df['state'] == 'failed'), (df['state'] == 'canceled'), (df['state'] == 'successful')  ]\n",
    "\n",
    "choices = [-1, 0, 1]\n",
    "df['Result'] = np.select(actual_conditions, choices, default=0)\n",
    "df['usd pledged'] = np.where(df['currency']=='USD', df['pledged'], df['usd pledged'])\n",
    "df['deadline'] = [x.year for x in df['deadline']]\n",
    "df['duration'] = (df['duration'] / np.timedelta64(1, 'D')).astype(int)\n",
    "df = df.drop(['pledged', 'state','country', 'launched'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "category = preprocessing.LabelEncoder()\n",
    "category.fit(df['category'])\n",
    "df['category'] = category.transform(df['category']) \n",
    "\n",
    "main_category = preprocessing.LabelEncoder()\n",
    "main_category.fit(df['main_category'])\n",
    "df['main_category'] = main_category.transform(df['main_category']) \n",
    "\n",
    "currency = preprocessing.LabelEncoder()\n",
    "currency.fit(df['currency'])\n",
    "df['currency'] = currency.transform(df['currency']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>backers</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>duration</th>\n",
       "      <th>pledged_percentage</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2421.000000</td>\n",
       "      <td>59</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.065795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>123</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>224</td>\n",
       "      <td>52375.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>1.047500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1205.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>40</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>58</td>\n",
       "      <td>8233.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>0.065864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>43</td>\n",
       "      <td>6240.570000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.096009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>12700.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>761</td>\n",
       "      <td>57763.779679</td>\n",
       "      <td>28</td>\n",
       "      <td>18.835000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>11</td>\n",
       "      <td>664.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>16</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.263333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>20</td>\n",
       "      <td>789.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>250.0</td>\n",
       "      <td>7</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1781.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>624</td>\n",
       "      <td>34268.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>1.713400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>12</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.180008</td>\n",
       "      <td>30</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>175.0</td>\n",
       "      <td>66</td>\n",
       "      <td>701.660000</td>\n",
       "      <td>20</td>\n",
       "      <td>4.009486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>147</td>\n",
       "      <td>15827.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>1.582700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>153</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>17757.0</td>\n",
       "      <td>571</td>\n",
       "      <td>43203.251145</td>\n",
       "      <td>30</td>\n",
       "      <td>2.754125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27</td>\n",
       "      <td>167.700306</td>\n",
       "      <td>30</td>\n",
       "      <td>1.123800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>840</td>\n",
       "      <td>57577.310000</td>\n",
       "      <td>30</td>\n",
       "      <td>1.151546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378631</th>\n",
       "      <td>106</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>413</td>\n",
       "      <td>13861.500000</td>\n",
       "      <td>30</td>\n",
       "      <td>1.386150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378632</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378633</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>97</td>\n",
       "      <td>11643.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.465720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378634</th>\n",
       "      <td>104</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>37</td>\n",
       "      <td>3531.000000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.882750</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378635</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>800.0</td>\n",
       "      <td>5</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378636</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>78</td>\n",
       "      <td>4814.877479</td>\n",
       "      <td>30</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378637</th>\n",
       "      <td>129</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>123</td>\n",
       "      <td>10245.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>1.024500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378638</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378639</th>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>103</td>\n",
       "      <td>5579.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>1.115800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378640</th>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378641</th>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378642</th>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>35</td>\n",
       "      <td>66.717700</td>\n",
       "      <td>27</td>\n",
       "      <td>1.246000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378643</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378644</th>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>120</td>\n",
       "      <td>6169.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>1.762571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378645</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>38</td>\n",
       "      <td>4281.148573</td>\n",
       "      <td>30</td>\n",
       "      <td>0.171125</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378646</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>950.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1732.020000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.823179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378647</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>4999.0</td>\n",
       "      <td>16</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.196039</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378648</th>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378649</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>500.0</td>\n",
       "      <td>6</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378650</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378651</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>78</td>\n",
       "      <td>5019.920882</td>\n",
       "      <td>29</td>\n",
       "      <td>1.100200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378652</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>2698.974335</td>\n",
       "      <td>30</td>\n",
       "      <td>1.075000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378653</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378654</th>\n",
       "      <td>130</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378655</th>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378656</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378657</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378658</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2010</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378659</th>\n",
       "      <td>138</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2016</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378660</th>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>17</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377602 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  main_category  currency  deadline      goal  backers  \\\n",
       "0            108             12         5      2015    1000.0        0   \n",
       "1             93              6        13      2017   30000.0       15   \n",
       "2             93              6        13      2013   45000.0        3   \n",
       "3             90             10        13      2012    5000.0        1   \n",
       "4             55              6        13      2015   19500.0       14   \n",
       "5            123              7        13      2016   50000.0      224   \n",
       "6             58              7        13      2014    1000.0       16   \n",
       "7             41              7        13      2016   25000.0       40   \n",
       "8            113              4        13      2014  125000.0       58   \n",
       "9             39              6        13      2014   65000.0       43   \n",
       "10            95             12         1      2013    2500.0        0   \n",
       "11            72             10        13      2013   12500.0      100   \n",
       "12            32              2        13      2014    5000.0        0   \n",
       "13            62              8        13      2016  200000.0        0   \n",
       "14           136              8         5      2017    5000.0      761   \n",
       "15            37              4        13      2015    2500.0       11   \n",
       "16            25              1        13      2014    1500.0       16   \n",
       "17            11             12        13      2015    3000.0       20   \n",
       "18            90             10        13      2012     250.0        7   \n",
       "19            58              7        13      2012    5000.0       40   \n",
       "20            52              5        13      2013   20000.0      624   \n",
       "21            20              5         0      2017    2500.0        1   \n",
       "22           141             14        13      2014    3500.0       12   \n",
       "23            41              7         9      2015     500.0        3   \n",
       "24            26              1        13      2014     175.0       66   \n",
       "25            90             10        13      2011   10000.0      147   \n",
       "26            34              2         5      2016   12000.0        0   \n",
       "27           153              6         1      2014   17757.0      571   \n",
       "28            26              1         5      2016     100.0       27   \n",
       "29             5              6        13      2017   50000.0      840   \n",
       "...          ...            ...       ...       ...       ...      ...   \n",
       "378631       106              8        13      2014   10000.0      413   \n",
       "378632        24              6        13      2014   12000.0        0   \n",
       "378633         7              5        13      2014   25000.0       97   \n",
       "378634       104             11        13      2011    4000.0       37   \n",
       "378635        95             12        13      2015     800.0        5   \n",
       "378636        30             10         1      2014    5000.0       78   \n",
       "378637       129              6        13      2017   10000.0      123   \n",
       "378638        95             12        13      2014    5000.0        5   \n",
       "378639        41              7        13      2015    5000.0      103   \n",
       "378640        54             12        13      2014    1000.0        2   \n",
       "378641       113              4         2      2015   80000.0        0   \n",
       "378642       136              8         4      2017    1000.0       35   \n",
       "378643         0             13        13      2014    6500.0        0   \n",
       "378644       113              4        13      2015    3500.0      120   \n",
       "378645        26              1         5      2013   16000.0       38   \n",
       "378646        19             12        13      2013     950.0       31   \n",
       "378647        95             12        13      2012    4999.0       16   \n",
       "378648       136              8        13      2016     500.0        5   \n",
       "378649        93              6        13      2017     500.0        6   \n",
       "378650         8             13        13      2016    6000.0        0   \n",
       "378651        23             10         1      2014    5000.0       78   \n",
       "378652        39              6         9      2015   20000.0       36   \n",
       "378653        39              6        13      2012    1700.0        1   \n",
       "378654       130              7        13      2017    6500.0        4   \n",
       "378655       108             12         1      2014    5500.0        0   \n",
       "378656        39              6        13      2014   50000.0        1   \n",
       "378657        93              6        13      2011    1500.0        5   \n",
       "378658        93              6        13      2010   15000.0        1   \n",
       "378659       138             13        13      2016   15000.0        6   \n",
       "378660        98              0        13      2011    2000.0       17   \n",
       "\n",
       "         usd pledged  duration  pledged_percentage  Result  \n",
       "0           0.000000        58            0.000000      -1  \n",
       "1        2421.000000        59            0.080700      -1  \n",
       "2         220.000000        45            0.004889      -1  \n",
       "3           1.000000        30            0.000200      -1  \n",
       "4        1283.000000        55            0.065795       0  \n",
       "5       52375.000000        35            1.047500       1  \n",
       "6        1205.000000        20            1.205000       1  \n",
       "7         453.000000        44            0.018120      -1  \n",
       "8        8233.000000        35            0.065864       0  \n",
       "9        6240.570000        30            0.096009       0  \n",
       "10          0.000000        30            0.000000      -1  \n",
       "11      12700.000000        30            1.016000       1  \n",
       "12          0.000000        30            0.000000      -1  \n",
       "13          0.000000        44            0.000000      -1  \n",
       "14      57763.779679        28           18.835000       1  \n",
       "15        664.000000        30            0.265600      -1  \n",
       "16        395.000000        30            0.263333      -1  \n",
       "17        789.000000        30            0.263000      -1  \n",
       "18        250.000000        15            1.000000       1  \n",
       "19       1781.000000        30            0.356200      -1  \n",
       "20      34268.000000        35            1.713400       1  \n",
       "21          0.000000        30            0.000400      -1  \n",
       "22        650.000000        29            0.185714      -1  \n",
       "23          6.180008        30            0.096000      -1  \n",
       "24        701.660000        20            4.009486       1  \n",
       "25      15827.000000        30            1.582700       1  \n",
       "26          0.000000        30            0.000000      -1  \n",
       "27      43203.251145        30            2.754125       1  \n",
       "28        167.700306        30            1.123800       1  \n",
       "29      57577.310000        30            1.151546       1  \n",
       "...              ...       ...                 ...     ...  \n",
       "378631  13861.500000        30            1.386150       1  \n",
       "378632      0.000000        29            0.000000      -1  \n",
       "378633  11643.000000        29            0.465720      -1  \n",
       "378634   3531.000000        61            0.882750      -1  \n",
       "378635     70.000000        30            0.087500      -1  \n",
       "378636   4814.877479        30            1.087000       1  \n",
       "378637  10245.000000        44            1.024500       1  \n",
       "378638     18.000000        31            0.003600      -1  \n",
       "378639   5579.000000        30            1.115800       1  \n",
       "378640    100.000000        33            0.100000       0  \n",
       "378641      0.000000        60            0.000000      -1  \n",
       "378642     66.717700        27            1.246000       1  \n",
       "378643      0.000000        21            0.000000       0  \n",
       "378644   6169.000000        21            1.762571       1  \n",
       "378645   4281.148573        30            0.171125      -1  \n",
       "378646   1732.020000        42            1.823179       1  \n",
       "378647    980.000000        30            0.196039      -1  \n",
       "378648    121.000000        30            0.242000       0  \n",
       "378649    135.000000        60            0.270000      -1  \n",
       "378650      0.000000        37            0.000000      -1  \n",
       "378651   5019.920882        29            1.100200       1  \n",
       "378652   2698.974335        30            1.075000       1  \n",
       "378653     25.000000        29            0.014706      -1  \n",
       "378654    154.000000        30            0.023692      -1  \n",
       "378655      0.000000        45            0.000000       0  \n",
       "378656     25.000000        30            0.000500       0  \n",
       "378657    155.000000        27            0.103333      -1  \n",
       "378658     20.000000        45            0.001333      -1  \n",
       "378659    200.000000        30            0.013333      -1  \n",
       "378660    524.000000        28            0.262000      -1  \n",
       "\n",
       "[377602 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(df.columns[0:-1])\n",
    "X = df[features]\n",
    "Y = df['Result']\n",
    "\n",
    "\n",
    "#Shuffle = False b/c gotta train on earlier data and test on later data \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=5, \n",
    "                                  min_samples_leaf=2, min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                  min_impurity_split=None, class_weight=None, presort=False)\n",
    "#DT = DT.fit(X_train, Y_train)\n",
    "#Ypreds = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84341502  0.83949407  0.84309881  0.84050593  0.84180237  0.84062737\n",
      "  0.83694896  0.83954209]\n",
      "Accuracy: 0.84 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "#The least populated class in y has only 8 members, which is too few. \n",
    "#The minimum number of members in any class cannot be less than n_splits=9.\n",
    "scores = cross_val_score(DT, X_train, Y_train, cv = 8)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning:\n",
    "\n",
    "min_samples_split : int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Networks (Multi-layer Perceptron (MLP) )\n",
    "\n",
    "Pro:\n",
    "*Capability to learn non-linear models.\n",
    "\n",
    "Con: \n",
    "*MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "\n",
    "*MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "\n",
    "*MLP is sensitive to feature scaling.\n",
    "\n",
    "\n",
    "## Features:\n",
    "1. hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
    "\n",
    "The ith element represents the number of neurons in the ith hidden layer.\n",
    "hidden_layer_sizes=(10, 2) : 10 hidden layer with 2 hidden units\n",
    "\n",
    "2. activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "Activation function for the hidden layer.\n",
    "\n",
    "3. solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "The solver for weight optimization.\n",
    "\n",
    "4. learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’\n",
    "Learning rate schedule for weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(hidden_layer_sizes=(20, 2), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n",
    " learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    " random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    " early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "#NN = NN.fit(X_train, Y_train) \n",
    "#Ypreds = NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62525692  0.62525692  0.62525692  0.62525692  0.62525692  0.62525692\n",
      "  0.62528163  0.62526682  0.62526682  0.62526682]\n",
      "Accuracy: 0.63 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NN, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "1. n_estimators : integer, optional (default=10)\n",
    "The number of trees in the forest.\n",
    "\n",
    "2. criterion : string, optional (default=”gini”)\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "\n",
    "3. max_features : int, float, string or None, optional (default=”auto”)\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "4. max_depth : integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "5. min_samples_split : int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node\n",
    "\n",
    "6. min_samples_leaf : int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99059289  0.98940711  0.99102767  0.99059289  0.99189723  0.99102767\n",
      "  0.98988102  0.99098743  0.99106649  0.98999921]\n",
      "Accuracy: 0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=10, criterion='gini', \n",
    "                            max_depth=None, min_samples_split=2, \n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                            max_features='auto', max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                            bootstrap=True, oob_score=False, n_jobs=1, \n",
    "                            random_state=None, verbose=0, \n",
    "                            warm_start=False, class_weight=None)\n",
    "\n",
    "#RF = RF.fit(X_train, Y_train) \n",
    "#Ypreds = RF.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(RF, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-at least 2 kernel functions\n",
    "\n",
    "Pros: \n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n",
    "\n",
    "## Features:\n",
    "\n",
    "1. Kernel(Linear, RBF, poly, sigmoid, precomputed)\n",
    "\n",
    "Linear kernel = Straight Line (hyperplane) as the decision boundary\n",
    "* rarely used in practice\n",
    "\n",
    "Radial Basis Function (RBF) = commonly used kernel in SVC\n",
    "2 parameters:\n",
    "* gamma\n",
    "* C\n",
    "\n",
    "2. Gamma:\n",
    "*  'spread' of the kernel and therefore the decision region.\n",
    "* low gamma -> the 'curve' of the decision boundary is very low and thus the decision region is very broad (underfitting)\n",
    "* gamma = 10 (The decision boundary starts to be highly effected by individual data points (i.e. variance)).\n",
    "* high gamma -> the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points (overfitting)\n",
    "* If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "* gamma defines how much influence a single training example has. \n",
    "* The larger gamma is, the closer other examples must be to be affected.\n",
    "\n",
    "3. C:\n",
    "* trades off misclassification of training examples against simplicity of the decision surface\n",
    "* A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly\n",
    "* penalty for misclassifying a data point\n",
    "* small C -> classifier is okay with misclassified data points (high bias, low variance)\n",
    "* big C -> classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)\n",
    "\n",
    "C > 10 is too slow\n",
    "\n",
    "4. degree : int, optional (default=3)\n",
    "* Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes FOREVER\n",
    "#linear 0.78\n",
    "#rbf 0.76\n",
    "#sigmoid 0.5\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "for i in kernels:\n",
    "    #for j, C in enumerate((0.01, 1, 10, 100)):\n",
    "        #for k, D in enumerate((1, 10)):\n",
    "    SVM = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "              decision_function_shape='ovr', degree=3, gamma='auto', kernel=i,\n",
    "              max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "    #SVM.fit(X_train, Y_train) \n",
    "    scores = cross_val_score(SVM, X_train, Y_train, cv = 10)\n",
    "\n",
    "    #print (\"Kernel: %s | Gamma: %0.2f | C: %i\" % (i, C, D))\n",
    "    print (\"Kernel: %s\" % (i))\n",
    "    print scores\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "                           leaf_size=30, p=2, metric='minkowski', metric_params=None, \n",
    "                           n_jobs=1)\n",
    "#KNN.fit(X_train, Y_train)\n",
    "scores = cross_val_score(KNN, X_train, Y_train, cv = 10)\n",
    "\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "1. weights : str or callable, optional (default = ‘uniform’)\n",
    "weight function used in prediction. Possible values:\n",
    "\n",
    "\n",
    "* uniform : uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "* distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Adjusted R Square\n",
    "\n",
    "ROC curve\n",
    "\n",
    "auc Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"Decision Tree\", \"Neural Network\", \"Boosting\", \"SVM\", \"KNN\"]\n",
    "\n",
    "#implement ROC Curves\n",
    "#\n",
    "\n",
    "h = .02 \n",
    "\n",
    "classifiers = [DT, NN, RF, SVM, KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
